{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b605089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 12:27:19.832298: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-24 12:27:20.829785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6cc1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras as keras\n",
    "from keras import layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c004aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, timeit\n",
    "from skimage.filters import threshold_otsu\n",
    "import numpy as np\n",
    "from math import inf as inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9300cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectral.io import envi as envi\n",
    "from spectral import imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee790ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf3717a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a946bbb-bf5b-41f6-b1e8-b704c45f0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dd26930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(physical_devices[1], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f72da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import platform\n",
    "DATA_DIRECTORY = \"\"\n",
    "SLASH = \"\"\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    DATA_DIRECTORY = \"/home/tyagi/Desktop/wheat/data/BULK/\"\n",
    "    SLASH = \"/\"\n",
    "elif platform == \"win32\":\n",
    "    DATA_DIRECTORY = \"D:\\wheat\\data\\BULK\\\\\"\n",
    "    SLASH=\"\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5979451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "BAND_NUMBER = 60\n",
    "FILLED_AREA_RATIO = 0.9\n",
    "TRAIN_IMAGE_COUNT = (1200/2) # 1200 images of 1 category for training. 600 from each B1 and B2\n",
    "VAL_IMAGE_COUNT = 400\n",
    "TEST_IMAGE_COUNT = 400\n",
    "NUM_VARIETIES = 4\n",
    "\n",
    "IMAGE_WIDTH = 30\n",
    "IMAGE_HEIGHT = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ac868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class filter_method(Enum):\n",
    "    none = 0\n",
    "    snv = 1\n",
    "    msc = 2\n",
    "    savgol = 3\n",
    "    \n",
    "FILTER = filter_method(0).name\n",
    "\n",
    "# to be set if filter chosen is savgol\n",
    "WINDOW = 7\n",
    "ORDER = 2\n",
    "DERIVATIVE = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82eeade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    " \n",
    "class feature_extraction_method(Enum):\n",
    "    none = 0\n",
    "    pca_loading = 1\n",
    "    lda = 2\n",
    "    ipca = 3\n",
    "\n",
    "FEATURE_EXTRACTION = feature_extraction_method(0).name\n",
    "\n",
    "NUM_OF_BANDS = 3\n",
    "if FEATURE_EXTRACTION == \"pca_loading\" or FEATURE_EXTRACTION == \"ipca\":\n",
    "    NUM_OF_BANDS = 8\n",
    "elif FEATURE_EXTRACTION == \"lda\":\n",
    "    NUM_OF_BANDS = 3\n",
    "    assert NUM_OF_BANDS <= min(NUM_VARIETIES-1,168),\"NUM_OF_BANDS is greater.\"\n",
    "\n",
    "\n",
    "REMOVE_NOISY_BANDS = False\n",
    "FIRST_BAND = 15\n",
    "LAST_BAND = 161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e61072a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_timer():\n",
    "    print(\"Testing started\")\n",
    "    return timeit.default_timer()\n",
    "\n",
    "def end_timer():\n",
    "    return timeit.default_timer()\n",
    "\n",
    "def show_time(tic,toc): \n",
    "    test_time = toc - tic\n",
    "    print('Time (s) = ' + str(test_time) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5e32422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exactPathHDR(variety,file):\n",
    "    return DATA_DIRECTORY+variety+SLASH+file+\".bil.hdr\"\n",
    "\n",
    "def exactPathBIL(variety,file):\n",
    "    return DATA_DIRECTORY+variety+SLASH+file+\".bil\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10991a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getROI(img, band_number):\n",
    "    img_band = img.read_band(band_number)\n",
    "    threshold = threshold_otsu(img_band)\n",
    "    roi=[]\n",
    "    for x in range(img_band.shape[0]):\n",
    "        a=[]\n",
    "        for y in range(img_band.shape[1]):\n",
    "            if img_band[x][y]>threshold:\n",
    "                a.append(1)\n",
    "            else:\n",
    "                a.append(0)\n",
    "        roi.append(a)\n",
    "    return roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f92ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns range for x and y from where we have to crop images\n",
    "def getRangeXandY(img,band_number):\n",
    "    img_band = img.read_band(band_number)\n",
    "    roi = getROI(img,band_number)\n",
    "    xmin = inf\n",
    "    xmax = 0\n",
    "    ymin = inf\n",
    "    ymax = 0\n",
    "    for x in range(img_band.shape[0]):\n",
    "        for y in range(img_band.shape[1]):\n",
    "            if roi[x][y]==1:\n",
    "                if x<xmin:\n",
    "                    xmin=x\n",
    "                if x>xmax:\n",
    "                    xmax=x\n",
    "                if y<ymin:\n",
    "                    ymin=y\n",
    "                if y>ymax:\n",
    "                    ymax=y\n",
    "    return xmin, xmax, ymin, ymax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09705166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCroppedImage(img,band_number):\n",
    "    xmin, xmax, ymin, ymax = getRangeXandY(img,band_number)\n",
    "    new_img = img[xmin:xmax, ymin:ymax, :]\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97d1f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCroppedROI(img,band_number):\n",
    "    xmin, xmax, ymin, ymax = getRangeXandY(img,band_number)\n",
    "    roi = np.array(getROI(img,band_number))\n",
    "    roi = roi[xmin:xmax, ymin:ymax]\n",
    "    return roi   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "318efc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUsefulImage(img,band_number):\n",
    "    crop_img = getCroppedImage(img,band_number)\n",
    "    crop_roi = getCroppedROI(img,band_number)\n",
    "    for x in range(crop_img.shape[2]):\n",
    "        band = crop_img[:,:,x]\n",
    "        crop_img[:,:,x] = band*crop_roi\n",
    "    return crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b7e5e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessHSI(img, band_number):\n",
    "    img = getUsefulImage(img, band_number)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad2376a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 12:27:25.023136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15047 MB memory:  -> device: 1, name: Quadro P5000, pci bus id: 0000:9b:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomCrop(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    layers.RandomRotation(factor=(-0.1, 0.1)),\n",
    "    layers.RandomZoom(height_factor=(-0.1, 0.1), width_factor=(-0.1,0.1)),\n",
    "    layers.RandomFlip(mode=\"horizontal_and_vertical\", seed=None)\n",
    "])\n",
    "\n",
    "def getAugumentedImage(img):\n",
    "    augmented_image = data_augmentation(img) \n",
    "    return augmented_image\n",
    "\n",
    "def checkAugumentedImage(img):\n",
    "    aug_band = img[:,:,0]\n",
    "    filled_area_ratio = (np.count_nonzero(aug_band))/(aug_band.shape[0]*aug_band.shape[1])\n",
    "    if filled_area_ratio > FILLED_AREA_RATIO :\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e16e403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List for All varieties\n",
    "VARIETIES = []\n",
    "VARIETIES_CODE = {}\n",
    "\n",
    "for name in os.listdir(DATA_DIRECTORY):\n",
    "    if (name.endswith(\".hdr\") or name.endswith(\".bil\")):\n",
    "        continue\n",
    "    VARIETIES_CODE[name] = len(VARIETIES)\n",
    "    VARIETIES.append(name)\n",
    "    if len(VARIETIES)==NUM_VARIETIES:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56b6e7ad-71ce-4d37-8ce9-4a725ac0ff7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HD 3086', 'PBW 291', 'DBW 187', 'DBW222']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VARIETIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20b64642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List for all file names in varities\n",
    "FILES = []\n",
    "MAX_FILE_NUM = 4\n",
    "for x in range(1,MAX_FILE_NUM+1):\n",
    "    FILES.append(\"B_\"+str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a072b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractRawImages(v):\n",
    "    #List of all images\n",
    "    images = []\n",
    "    for f in FILES:\n",
    "        try:\n",
    "            img = envi.open(exactPathHDR(v,f),exactPathBIL(v,f))\n",
    "            img = preprocessHSI(img, BAND_NUMBER)\n",
    "            images.append(img)\n",
    "        except:\n",
    "            pass\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c910c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def createDataset(images, label):\n",
    "    train_dataset = []\n",
    "    train_dataset_label = []\n",
    "    val_dataset = []\n",
    "    val_dataset_label = []\n",
    "    test_dataset = []\n",
    "    test_dataset_label = []\n",
    "    tic = start_timer()\n",
    "    for index, img in enumerate(images):\n",
    "        count = 0\n",
    "        if index == 0 or index ==1:\n",
    "            while count<TRAIN_IMAGE_COUNT:\n",
    "                aug_img = getAugumentedImage(img)\n",
    "                if checkAugumentedImage(aug_img):\n",
    "                    train_dataset.append(aug_img)\n",
    "                    train_dataset_label.append(label)\n",
    "                    count+=1 \n",
    "                clear_output(wait=True)\n",
    "                print(\"Label: \",label,\" Index: \",index,\" Count: \",count)\n",
    "        elif index == 2:\n",
    "            while count<VAL_IMAGE_COUNT:\n",
    "                aug_img = getAugumentedImage(img)\n",
    "                if checkAugumentedImage(aug_img):\n",
    "                    val_dataset.append(aug_img)\n",
    "                    val_dataset_label.append(label)\n",
    "                    count+=1 \n",
    "                clear_output(wait=True)\n",
    "                print(\"Label: \",label,\" Index: \",index,\" Count: \",count)\n",
    "        elif index ==3:\n",
    "            while count<TEST_IMAGE_COUNT:\n",
    "                aug_img = getAugumentedImage(img)\n",
    "                if checkAugumentedImage(aug_img):\n",
    "                    test_dataset.append(aug_img)\n",
    "                    test_dataset_label.append(label)\n",
    "                    count+=1 \n",
    "                clear_output(wait=True)\n",
    "                print(\"Label: \",label,\" Index: \",index,\" Count: \",count)\n",
    "        else:\n",
    "            print(\"Something Wrong\")\n",
    "    toc = end_timer()\n",
    "    show_time(tic,toc)\n",
    "    \n",
    "    train_dataset = np.array(train_dataset)\n",
    "    train_dataset_label = np.array([VARIETIES_CODE[label] for label in train_dataset_label])\n",
    "    val_dataset = np.array(val_dataset)\n",
    "    val_dataset_label = np.array([VARIETIES_CODE[label] for label in val_dataset_label])\n",
    "    test_dataset = np.array(test_dataset)\n",
    "    test_dataset_label = np.array([VARIETIES_CODE[label] for label in test_dataset_label])\n",
    "    \n",
    "    return train_dataset,train_dataset_label,val_dataset,val_dataset_label,test_dataset,test_dataset_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28a9b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = os.path.join('./dataset')\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8dfbb4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_file_name(variety):\n",
    "    name = \"./dataset/V\"+str(variety).zfill(3)+\"_FilledArea_\"+str(FILLED_AREA_RATIO)+\"_NumOfBands_\"+str(NUM_OF_BANDS)+\"_FB_\"+str(FIRST_BAND)+\"_LB_\"+str(LAST_BAND)+\"_BandNo_\"+str(BAND_NUMBER)+\"_ImageHeight_\"+str(IMAGE_HEIGHT)+\"_ImageWidth_\"+str(IMAGE_WIDTH)+\"_FILTER_\"+str(FILTER)+\"_FeatureExtraction_\"+str(FEATURE_EXTRACTION)\n",
    "    if REMOVE_NOISY_BANDS:\n",
    "        name+=\"_REMOVE_NOISY_BANDS_\"+str(REMOVE_NOISY_BANDS)\n",
    "    if FILTER == \"savgol\":\n",
    "        name+=\"_WINDOW_\"+str(WINDOW)+\"_ORDER_\"+str(ORDER)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d86b1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(variety, train_dataset,train_dataset_label,val_dataset,val_dataset_label,test_dataset,test_dataset_label):\n",
    "    DATASET_FILE_NAME = dataset_file_name(variety)\n",
    "    np.save(DATASET_FILE_NAME+\"_train_dataset.npy\",train_dataset)\n",
    "    np.save(DATASET_FILE_NAME+\"_train_dataset_label.npy\",train_dataset_label)\n",
    "    np.save(DATASET_FILE_NAME+\"_val_dataset.npy\",val_dataset)\n",
    "    np.save(DATASET_FILE_NAME+\"_val_dataset_label.npy\",val_dataset_label)\n",
    "    np.save(DATASET_FILE_NAME+\"_test_dataset.npy\",test_dataset)\n",
    "    np.save(DATASET_FILE_NAME+\"_test_dataset_label.npy\",test_dataset_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7deebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noisy_bands(remove_noisy_bands,train_dataset,val_dataset,test_dataset):\n",
    "    if remove_noisy_bands:\n",
    "        train_dataset = train_dataset[:,:,:,FIRST_BAND:LAST_BAND+1]\n",
    "        val_dataset = val_dataset[:,:,:,FIRST_BAND:LAST_BAND+1]\n",
    "        test_dataset = test_dataset[:,:,:,FIRST_BAND:LAST_BAND+1]\n",
    "    return train_dataset,val_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd9ea6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snv(input_data):\n",
    "    \"\"\"\n",
    "        :snv: A correction technique which is done on each\n",
    "        individual spectrum, a reference spectrum is not\n",
    "        required        :param input_data: Array of spectral data\n",
    "        :type input_data: DataFrame\n",
    "\n",
    "        :returns: data_snv (ndarray): Scatter corrected spectra\n",
    "    \"\"\"\n",
    "\n",
    "    input_data = np.asarray(input_data)\n",
    "\n",
    "    # Define a new array and populate it with the corrected data  \n",
    "    data_snv = np.zeros_like(input_data)\n",
    "    for i in range(data_snv.shape[0]):    # Apply correction\n",
    "        data_snv[i,:] = (input_data[i,:] - np.mean(input_data[i,:])) / np.std(input_data[i,:])\n",
    "    return (data_snv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9b221c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def msc(input_data, reference=None):\n",
    "    ''' Perform Multiplicative scatter correction'''\n",
    "\n",
    "    # mean centre correction\n",
    "    for i in range(input_data.shape[0]):\n",
    "        input_data[i,:] -= input_data[i,:].mean()\n",
    "\n",
    "    # Get the reference spectrum. If not given, estimate it from the mean    \n",
    "    if reference is None:\n",
    "        # Calculate mean\n",
    "        ref = np.mean(input_data, axis=0)\n",
    "    else:\n",
    "        ref = reference\n",
    "#     print(ref.shape)\n",
    "#     print(input.shape)\n",
    "\n",
    "    ref = np.reshape(ref,-1)\n",
    "    \n",
    "    # Define a new array and populate it with the corrected data    \n",
    "    data_msc = np.zeros_like(input_data)\n",
    "    for i in range(input_data.shape[0]):\n",
    "        # Run regression\n",
    "        in_data = np.reshape(input_data[i,:], -1)\n",
    "        fit = np.polyfit(ref, in_data, 1, full=True)\n",
    "        # Apply correction\n",
    "        data_msc[i,:] = (input_data[i,:] - fit[0][1]) / fit[0][0] \n",
    "\n",
    "    return (data_msc, ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bf6d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "def sgolay2d (input_data, window_size, order, derivative=\"none\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # number of terms in the polynomial expression\n",
    "    n_terms = ( order + 1 ) * ( order + 2)  / 2.0\n",
    "\n",
    "    if  window_size % 2 == 0:\n",
    "        raise ValueError('window_size must be odd')\n",
    "\n",
    "    if window_size**2 < n_terms:\n",
    "        raise ValueError('order is too high for the window size')\n",
    "\n",
    "    half_size = window_size // 2\n",
    "\n",
    "    # exponents of the polynomial. \n",
    "    # p(x,y) = a0 + a1*x + a2*y + a3*x^2 + a4*y^2 + a5*x*y + ... \n",
    "    # this line gives a list of two item tuple. Each tuple contains \n",
    "    # the exponents of the k-th term. First element of tuple is for x\n",
    "    # second element for y.\n",
    "    # Ex. exps = [(0,0), (1,0), (0,1), (2,0), (1,1), (0,2), ...]\n",
    "    exps = [ (k-n, n) for k in range(order+1) for n in range(k+1) ]\n",
    "\n",
    "    # coordinates of points\n",
    "    ind = np.arange(-half_size, half_size+1, dtype=np.float64)\n",
    "    dx = np.repeat( ind, window_size )\n",
    "    dy = np.tile( ind, [window_size, 1]).reshape(window_size**2, )\n",
    "\n",
    "    # build matrix of system of equation\n",
    "    A = np.empty( (window_size**2, len(exps)) )\n",
    "    for i, exp in enumerate( exps ):\n",
    "        A[:,i] = (dx**exp[0]) * (dy**exp[1])\n",
    "    \n",
    "    filtered_data = np.empty(shape = input_data.shape)\n",
    "    \n",
    "    for num, input_ in enumerate(input_data):\n",
    "        filtered_image = np.empty(shape = input_.shape)\n",
    "        for i in range(input_.shape[2]):\n",
    "            z = input_[:,:,i]\n",
    "            # pad input array with appropriate values at the four borders\n",
    "            new_shape = z.shape[0] + 2*half_size, z.shape[1] + 2*half_size\n",
    "            Z = np.zeros( (new_shape) )\n",
    "            # top band\n",
    "            band = z[0, :]\n",
    "            Z[:half_size, half_size:-half_size] =  band -  np.abs( np.flipud( z[1:half_size+1, :] ) - band )\n",
    "            # bottom band\n",
    "            band = z[-1, :]\n",
    "            Z[-half_size:, half_size:-half_size] = band  + np.abs( np.flipud( z[-half_size-1:-1, :] )  -band )\n",
    "            # left band\n",
    "            band = np.tile( z[:,0].reshape(-1,1), [1,half_size])\n",
    "            Z[half_size:-half_size, :half_size] = band - np.abs( np.fliplr( z[:, 1:half_size+1] ) - band )\n",
    "            # right band\n",
    "            band = np.tile( z[:,-1].reshape(-1,1), [1,half_size] )\n",
    "            Z[half_size:-half_size, -half_size:] =  band + np.abs( np.fliplr( z[:, -half_size-1:-1] ) - band )\n",
    "            # central band\n",
    "            Z[half_size:-half_size, half_size:-half_size] = z\n",
    "\n",
    "            # top left corner\n",
    "            band = z[0,0]\n",
    "            Z[:half_size,:half_size] = band - np.abs( np.flipud(np.fliplr(z[1:half_size+1,1:half_size+1]) ) - band )\n",
    "            # bottom right corner\n",
    "            band = z[-1,-1]\n",
    "            Z[-half_size:,-half_size:] = band + np.abs( np.flipud(np.fliplr(z[-half_size-1:-1,-half_size-1:-1]) ) - band )\n",
    "\n",
    "            # top right corner\n",
    "            band = Z[half_size,-half_size:]\n",
    "            Z[:half_size,-half_size:] = band - np.abs( np.flipud(Z[half_size+1:2*half_size+1,-half_size:]) - band )\n",
    "            # bottom left corner\n",
    "            band = Z[-half_size:,half_size].reshape(-1,1)\n",
    "            Z[-half_size:,:half_size] = band - np.abs( np.fliplr(Z[-half_size:, half_size+1:2*half_size+1]) - band )\n",
    "\n",
    "            # solve system and convolve\n",
    "            if derivative == \"none\":\n",
    "                m = np.linalg.pinv(A)[0].reshape((window_size, -1))\n",
    "                filtered_image[:,:,i] = scipy.signal.fftconvolve(Z, m, mode='valid')\n",
    "            elif derivative == 'col':\n",
    "                c = np.linalg.pinv(A)[1].reshape((window_size, -1))\n",
    "                filtered_image[:,:,i] = scipy.signal.fftconvolve(Z, -c, mode='valid')\n",
    "            elif derivative == 'row':\n",
    "                r = np.linalg.pinv(A)[2].reshape((window_size, -1))\n",
    "                filtered_image[:,:,i] = scipy.signal.fftconvolve(Z, -r, mode='valid')\n",
    "            elif derivative == 'both':\n",
    "                c = np.linalg.pinv(A)[1].reshape((window_size, -1))\n",
    "                r = np.linalg.pinv(A)[2].reshape((window_size, -1))\n",
    "                filtered_image[:,:,i] = scipy.signal.fftconvolve(Z, -r, mode='valid'), scipy.signal.fftconvolve(Z, -c, mode='valid')\n",
    "        filtered_data[num, :, :, :] = filtered_image\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e493dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filters(input_data, reference=None):\n",
    "    if FILTER == \"snv\":\n",
    "        return snv(input_data)\n",
    "    elif FILTER == \"msc\":\n",
    "        return msc(input_data, reference)[0]\n",
    "    elif FILTER == \"savgol\":\n",
    "        return sgolay2d(input_data, window_size = WINDOW, order = ORDER, derivative= DERIVATIVE)\n",
    "    else:\n",
    "        return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4afcc20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def lda(X_train,Y_train,X_test, numComponents = NUM_OF_BANDS):\n",
    "    \n",
    "    assert numComponents <= min(NUM_VARIETIES-1,X_train.shape[3]),\"NUM_OF_BANDS is greater.\"\n",
    "    \n",
    "    RX_train = np.reshape(X_train, (-1, X_train.shape[3]))\n",
    "    RX_test = np.reshape(X_test, (-1, X_test.shape[3]))\n",
    "    RY_train = []\n",
    "    for i in range(Y_train.shape[0]):\n",
    "        for x in range(X_train.shape[1]*X_train.shape[2]):\n",
    "            RY_train.append(Y_train[i])\n",
    "    RY_train = np.array(RY_train)\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis(n_components=numComponents)\n",
    "    RX_train = lda.fit_transform(RX_train, RY_train)\n",
    "    RX_test = lda.transform(RX_test)\n",
    "    \n",
    "    X_train = np.reshape(RX_train, (-1,X_train.shape[1],X_train.shape[2], numComponents))\n",
    "    X_test = np.reshape(RX_test, (-1,X_test.shape[1],X_test.shape[2], numComponents))\n",
    "    \n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2650db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_loading(inp,numComponents = NUM_OF_BANDS):\n",
    "    t = inp.reshape(-1, inp.shape[2])\n",
    "    pca = PCA(n_components = numComponents)\n",
    "    dt = pca.fit_transform(t)\n",
    "    dt = dt.reshape(inp.shape[0],inp.shape[1],-1)\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6d66676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#just for checking the number of bands to take into account. 99.97% is good enough to consider.\n",
    "def check_pca_bands(inp):\n",
    "    t = inp.reshape(-1, inp.shape[2])\n",
    "    pca = PCA(n_components = 75)\n",
    "    principalComponents = pca.fit_transform(t)\n",
    "    ev=pca.explained_variance_ratio_\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(np.cumsum(ev))\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Cumulative explained variance')\n",
    "    plt.show()\n",
    "    \n",
    "    return np.cumsum(ev)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0583d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dimensional Reduction Method\n",
    "def ipca(HSI, numComponents = NUM_OF_BANDS):\n",
    "    print(HSI.shape)\n",
    "    RHSI = np.reshape(HSI, (-1, HSI.shape[2]))\n",
    "    print(RHSI.shape)\n",
    "    n_batches = 10\n",
    "    inc_pca = IncrementalPCA(n_components=numComponents)\n",
    "    for X_batch in np.array_split(RHSI, n_batches):\n",
    "        inc_pca.partial_fit(X_batch)\n",
    "    X_ipca = inc_pca.transform(RHSI)\n",
    "    print(X_ipca.shape)\n",
    "    RHSI = np.reshape(X_ipca, (HSI.shape[0],HSI.shape[1], -1))\n",
    "    print(RHSI.shape)\n",
    "    return RHSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ffd8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(X_train,Y_train,X_test,Y_test,method=\"none\"):\n",
    "    if method==\"none\":\n",
    "        pass\n",
    "    elif method == \"pca_loading\":\n",
    "        X_train = np.array([pca_loading(inp) for inp in X_train])\n",
    "        X_test = np.array([pca_loading(inp) for inp in X_test])\n",
    "    elif method == \"lda\":\n",
    "        X_train,X_test = lda(X_train,Y_train,X_test)\n",
    "    elif method == \"ipca\":\n",
    "        X_train = np.array([ipca(inp) for inp in X_train])\n",
    "        X_test = np.array([ipca(inp) for inp in X_test])\n",
    "    \n",
    "    return X_train,Y_train,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1317f008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  DBW222  Index:  3  Count:  400\n",
      "Time (s) = 484.6707456250151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for v in VARIETIES:\n",
    "    images = extractRawImages(v)\n",
    "    train_dataset,train_dataset_label,val_dataset,val_dataset_label,test_dataset,test_dataset_label = createDataset(images, v)\n",
    "    train_dataset,val_dataset,test_dataset = remove_noisy_bands(REMOVE_NOISY_BANDS,train_dataset,val_dataset,test_dataset)\n",
    "    train_dataset = apply_filters(train_dataset)\n",
    "    val_dataset = apply_filters(val_dataset)\n",
    "    test_dataset = apply_filters(test_dataset)\n",
    "    save_dataset(v, train_dataset,train_dataset_label,val_dataset,val_dataset_label,test_dataset,test_dataset_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7bf6c-6a87-4fd2-96e3-0760b611087a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwheatenv",
   "language": "python",
   "name": "dwheatenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
